# MSc-Dissertation

In this project, the text analysis and music generation methods are combined to realize
an emotion-conditioned text-to-music system, aiming to discover the emotional connection
between text and music. A text emotion analysis module developed with traditional natural
language processing techniques and two emotion-conditioned music generation methods are
presented. Both music generation models - EMOPIA (baseline)
and Artificial Musical Intelligence (AMI) are transformer-based models with an emotion token
introduced for emotional information processing. Additionally, AMI applies an instrument
token and two structural tokens to generate multi-instruments music with a consistent
rhythm and a coherent structure. Moreover, an emotion-labelled MIDI dataset MYEMO,
containing 400 movie soundtracks, is proposed and applied to the fine-tuning of the AMI
model. According to objective evaluation and the survey results, compared with the CWT
model, the AMI model is capable of music generation with emotion closer to that of the text
and with higher overall music quality. In addition, the MYEMO dataset is able to further
improve music quality and emotion accuracy.
